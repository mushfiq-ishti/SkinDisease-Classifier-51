{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Importing Libraries</b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-23T14:39:15.135387Z",
     "iopub.status.busy": "2025-08-23T14:39:15.134659Z",
     "iopub.status.idle": "2025-08-23T14:41:16.179695Z",
     "shell.execute_reply": "2025-08-23T14:41:16.179032Z",
     "shell.execute_reply.started": "2025-08-23T14:39:15.135355Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         os.path.join(dirname, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T04:06:56.863153Z",
     "iopub.status.busy": "2025-08-31T04:06:56.862999Z",
     "iopub.status.idle": "2025-08-31T04:07:17.233827Z",
     "shell.execute_reply": "2025-08-31T04:07:17.233078Z",
     "shell.execute_reply.started": "2025-08-31T04:06:56.863138Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix , accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, matthews_corrcoef, confusion_matrix, accuracy_score\n",
    "#from imblearn.metrics import geometric_mean_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Datset Loading</b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T04:07:17.236295Z",
     "iopub.status.busy": "2025-08-31T04:07:17.235605Z",
     "iopub.status.idle": "2025-08-31T04:07:17.239886Z",
     "shell.execute_reply": "2025-08-31T04:07:17.239081Z",
     "shell.execute_reply.started": "2025-08-31T04:07:17.236273Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_path = '/kaggle/input/50-skin-disease/Best_50_class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T04:07:17.240893Z",
     "iopub.status.busy": "2025-08-31T04:07:17.240656Z",
     "iopub.status.idle": "2025-08-31T04:07:38.109137Z",
     "shell.execute_reply": "2025-08-31T04:07:38.108389Z",
     "shell.execute_reply.started": "2025-08-31T04:07:17.240872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root= dataset_path)\n",
    "class_names = dataset.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T04:07:38.110095Z",
     "iopub.status.busy": "2025-08-31T04:07:38.109892Z",
     "iopub.status.idle": "2025-08-31T04:07:38.115677Z",
     "shell.execute_reply": "2025-08-31T04:07:38.114986Z",
     "shell.execute_reply.started": "2025-08-31T04:07:38.110071Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# dataset.targets has numeric labels for each image\n",
    "counts = Counter(dataset.targets)\n",
    "\n",
    "# Map counts to class names\n",
    "for class_idx, count in counts.items():\n",
    "    print(f\"{dataset.classes[class_idx]}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T04:07:38.116889Z",
     "iopub.status.busy": "2025-08-31T04:07:38.116566Z",
     "iopub.status.idle": "2025-08-31T04:07:38.140283Z",
     "shell.execute_reply": "2025-08-31T04:07:38.139717Z",
     "shell.execute_reply.started": "2025-08-31T04:07:38.116866Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_counts = pd.DataFrame({\n",
    "    \"Class\": [dataset.classes[idx] for idx in counts.keys()],\n",
    "    \"Count\": [counts[idx] for idx in counts.keys()]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df_counts.to_csv(\"class_counts.csv\", index=False)\n",
    "\n",
    "print(\"Counts saved to class_counts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Vision Transformer (ViT)-Tensforflow</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Dataset Splitting</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T14:42:06.916102Z",
     "iopub.status.busy": "2025-08-23T14:42:06.915444Z",
     "iopub.status.idle": "2025-08-23T14:42:16.906000Z",
     "shell.execute_reply": "2025-08-23T14:42:16.905197Z",
     "shell.execute_reply.started": "2025-08-23T14:42:06.916073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "data_dir = dataset_path\n",
    "img_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# =========================\n",
    "# Step 1: 70% training set\n",
    "# =========================\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.30,   # leave 30% for val+test\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Step 2: 30% val+test pool\n",
    "# =========================\n",
    "val_test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.30,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Step 3: Split val_test_ds into 15% val + 15% test\n",
    "# =========================\n",
    "val_test_size = val_test_ds.cardinality().numpy()\n",
    "val_size = val_test_size // 2\n",
    "test_size = val_test_size - val_size  # handles odd numbers safely\n",
    "\n",
    "val_ds = val_test_ds.take(val_size)\n",
    "test_ds = val_test_ds.skip(val_size)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T14:42:16.906947Z",
     "iopub.status.busy": "2025-08-23T14:42:16.906741Z",
     "iopub.status.idle": "2025-08-23T14:42:16.912740Z",
     "shell.execute_reply": "2025-08-23T14:42:16.911921Z",
     "shell.execute_reply.started": "2025-08-23T14:42:16.906930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32  # Use the same batch size you set\n",
    "\n",
    "def dataset_size(dataset):\n",
    "    # Get number of batches\n",
    "    batches = dataset.cardinality().numpy()\n",
    "    if batches == tf.data.INFINITE_CARDINALITY or batches == tf.data.UNKNOWN_CARDINALITY:\n",
    "        return \"Unknown size\"\n",
    "    else:\n",
    "        return batches * batch_size\n",
    "\n",
    "print(\"Train set size:\", dataset_size(train_ds))\n",
    "print(\"Validation set size:\", dataset_size(val_ds))\n",
    "print(\"Test set size:\", dataset_size(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Model Compilation</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T14:42:16.913871Z",
     "iopub.status.busy": "2025-08-23T14:42:16.913609Z",
     "iopub.status.idle": "2025-08-23T14:42:22.596689Z",
     "shell.execute_reply": "2025-08-23T14:42:22.595543Z",
     "shell.execute_reply.started": "2025-08-23T14:42:16.913848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install vit-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T14:42:22.600159Z",
     "iopub.status.busy": "2025-08-23T14:42:22.599871Z",
     "iopub.status.idle": "2025-08-23T14:42:28.734745Z",
     "shell.execute_reply": "2025-08-23T14:42:28.734131Z",
     "shell.execute_reply.started": "2025-08-23T14:42:22.600135Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from vit_keras import vit\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# =========================\n",
    "# Configuration\n",
    "# =========================\n",
    "num_classes = len(class_names)\n",
    "image_size = 224  # ViT was pretrained on 224x224\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# =========================\n",
    "# Backbone (ViT)\n",
    "# =========================\n",
    "# vit-keras does not allow include_top=False with pretrained=True\n",
    "# Instead, load with pretrained weights, then remove the head manually.\n",
    "vit_base = vit.vit_b16(\n",
    "    image_size=image_size,\n",
    "    pretrained=True,\n",
    "    include_top=True,     # must be True if pretrained=True\n",
    "    pretrained_top=True   # load the pretrained head\n",
    ")\n",
    "\n",
    "# Remove the classification head (last layer)\n",
    "vit_base = models.Model(\n",
    "    inputs=vit_base.input,\n",
    "    outputs=vit_base.layers[-2].output   # take features before final Dense\n",
    ")\n",
    "\n",
    "# Freeze the backbone\n",
    "vit_base.trainable = False\n",
    "\n",
    "# =========================\n",
    "# Custom classification head\n",
    "# =========================\n",
    "x = vit_base.output\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "output = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "model = models.Model(inputs=vit_base.input, outputs=output)\n",
    "\n",
    "# =========================\n",
    "# Compile\n",
    "# =========================\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T14:42:28.735686Z",
     "iopub.status.busy": "2025-08-23T14:42:28.735447Z",
     "iopub.status.idle": "2025-08-23T14:42:28.768759Z",
     "shell.execute_reply": "2025-08-23T14:42:28.768215Z",
     "shell.execute_reply.started": "2025-08-23T14:42:28.735669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T14:42:28.769703Z",
     "iopub.status.busy": "2025-08-23T14:42:28.769401Z",
     "iopub.status.idle": "2025-08-23T14:42:28.777589Z",
     "shell.execute_reply": "2025-08-23T14:42:28.776886Z",
     "shell.execute_reply.started": "2025-08-23T14:42:28.769684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#sections to modify\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',      # Metric to monitor\n",
    "    patience=5,              # Number of epochs with no improvement to wait\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with best val_loss\n",
    ")\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_model_vit.h5',        # File path to save the model\n",
    "    monitor='val_loss',     # Metric to monitor\n",
    "    save_best_only=True,    # Save only when improvement\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Model Training</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T14:42:28.778797Z",
     "iopub.status.busy": "2025-08-23T14:42:28.778299Z",
     "iopub.status.idle": "2025-08-23T14:47:03.394646Z",
     "shell.execute_reply": "2025-08-23T14:47:03.393962Z",
     "shell.execute_reply.started": "2025-08-23T14:42:28.778771Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping, checkpoint]   # Include both callbacks here\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Results</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T14:47:03.395813Z",
     "iopub.status.busy": "2025-08-23T14:47:03.395569Z",
     "iopub.status.idle": "2025-08-23T14:47:03.403091Z",
     "shell.execute_reply": "2025-08-23T14:47:03.402308Z",
     "shell.execute_reply.started": "2025-08-23T14:47:03.395787Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hist_training_vit=pd.DataFrame(history.history)\n",
    "hist_training_vit\n",
    "# Save to CSV\n",
    "hist_training_vit.to_csv(\"hist_training_vit.csv\", index=False)\n",
    "print(\"ViT Training history to hist_training_vit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T14:47:03.404210Z",
     "iopub.status.busy": "2025-08-23T14:47:03.403963Z",
     "iopub.status.idle": "2025-08-23T14:47:07.568608Z",
     "shell.execute_reply": "2025-08-23T14:47:07.567765Z",
     "shell.execute_reply.started": "2025-08-23T14:47:03.404193Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#sections to modify\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "num_epochs = len(history.history['accuracy'])  # total epochs trained\n",
    "\n",
    "# X ticks positions (integers from 0 to num_epochs, step 4)\n",
    "xticks = range(0, num_epochs + 1, 4)\n",
    "\n",
    "# Plot accuracy\n",
    "axs[0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "axs[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axs[0].set_xlabel('Epoch', fontsize=20)\n",
    "axs[0].set_ylabel('Accuracy', fontsize=20)\n",
    "axs[0].set_title('Training and Validation Accuracy - ViT', fontsize=22)\n",
    "axs[0].legend(fontsize=18)\n",
    "axs[0].set_xticks(xticks)\n",
    "axs[0].tick_params(axis='x', labelsize=18)\n",
    "axs[0].tick_params(axis='y', labelsize=18)\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Plot loss\n",
    "axs[1].plot(history.history['loss'], label='Train Loss')\n",
    "axs[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axs[1].set_xlabel('Epoch', fontsize=20)\n",
    "axs[1].set_ylabel('Loss', fontsize=20)\n",
    "axs[1].set_title('Training and Validation Loss - ViT', fontsize=22)\n",
    "axs[1].legend(fontsize=18)\n",
    "axs[1].set_xticks(xticks)\n",
    "axs[1].tick_params(axis='x', labelsize=18)\n",
    "axs[1].tick_params(axis='y', labelsize=18)\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the combined figure\n",
    "plt.savefig('vit_training_curves.png', dpi=600)\n",
    "plt.savefig('vit_training_curves.pdf')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T14:47:07.570259Z",
     "iopub.status.busy": "2025-08-23T14:47:07.569484Z",
     "iopub.status.idle": "2025-08-23T14:48:27.564665Z",
     "shell.execute_reply": "2025-08-23T14:48:27.563806Z",
     "shell.execute_reply.started": "2025-08-23T14:47:07.570239Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, matthews_corrcoef\n",
    "\n",
    "# 1. Get true and predicted labels\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in test_ds:\n",
    "    preds = model.predict(images)\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    y_pred.extend(np.argmax(preds, axis=1))\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# 2. Compute confusion matrix and metrics\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f'Matthews Correlation Coefficient: {mcc:.4f}')\n",
    "\n",
    "# 3. Plot confusion matrix\n",
    "plt.figure(figsize=(30, 25))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels= class_names, yticklabels= class_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('vit_confusion_matrix.png', dpi=600)\n",
    "plt.savefig('vit_confusion_matrix.pdf')\n",
    "plt.show()\n",
    "\n",
    "# 4. Save metrics to CSV\n",
    "metrics = {\n",
    "    'Accuracy': [accuracy],\n",
    "    'Recall': [recall],\n",
    "    'Precision': [precision],\n",
    "    'F1 Score': [f1],\n",
    "    'MCC':[mcc]\n",
    "}\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "df_cm.to_csv(\"confusion_vit.csv\")\n",
    "\n",
    "df_metrics_vit = pd.DataFrame(metrics)\n",
    "df_metrics_vit.to_csv('performance_metrics_vit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Swin Transformer</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Importing Librarires</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T04:08:30.576107Z",
     "iopub.status.busy": "2025-08-31T04:08:30.575424Z",
     "iopub.status.idle": "2025-08-31T04:08:30.579999Z",
     "shell.execute_reply": "2025-08-31T04:08:30.579345Z",
     "shell.execute_reply.started": "2025-08-31T04:08:30.576083Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, matthews_corrcoef\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Dataset Loading</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T04:08:33.295092Z",
     "iopub.status.busy": "2025-08-31T04:08:33.294853Z",
     "iopub.status.idle": "2025-08-31T04:08:33.388394Z",
     "shell.execute_reply": "2025-08-31T04:08:33.387664Z",
     "shell.execute_reply.started": "2025-08-31T04:08:33.295076Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"/kaggle/input/50-skin-disease/Best_50_class\"   # dataset root folder (class subfolders inside)\n",
    "img_size = 224            # Swin expects 224x224\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "learning_rate = 1e-4\n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T04:08:35.592084Z",
     "iopub.status.busy": "2025-08-31T04:08:35.591840Z",
     "iopub.status.idle": "2025-08-31T04:08:44.788800Z",
     "shell.execute_reply": "2025-08-31T04:08:44.788225Z",
     "shell.execute_reply.started": "2025-08-31T04:08:35.592068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms   # ‚úÖ this line\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],   # ImageNet normalization\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "num_classes = len(full_dataset.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Dataset Splitting</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T04:08:44.790176Z",
     "iopub.status.busy": "2025-08-31T04:08:44.789928Z",
     "iopub.status.idle": "2025-08-31T04:08:44.797369Z",
     "shell.execute_reply": "2025-08-31T04:08:44.796756Z",
     "shell.execute_reply.started": "2025-08-31T04:08:44.790154Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_size = len(full_dataset)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size   = int(0.15 * dataset_size)\n",
    "test_size  = dataset_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(seed)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T04:08:44.798245Z",
     "iopub.status.busy": "2025-08-31T04:08:44.798003Z",
     "iopub.status.idle": "2025-08-31T04:08:53.047660Z",
     "shell.execute_reply": "2025-08-31T04:08:53.047125Z",
     "shell.execute_reply.started": "2025-08-31T04:08:44.798229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# full dataset\n",
    "full_dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "# save class names once\n",
    "class_names = full_dataset.classes   # ‚úÖ store here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Compiling</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T08:36:13.558929Z",
     "iopub.status.busy": "2025-08-29T08:36:13.558236Z",
     "iopub.status.idle": "2025-08-29T08:36:15.347325Z",
     "shell.execute_reply": "2025-08-29T08:36:15.346596Z",
     "shell.execute_reply.started": "2025-08-29T08:36:13.558904Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "model = models.swin_t(weights=\"IMAGENET1K_V1\")  # Swin Tiny pretrained\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)  # replace classifier\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-29T08:38:13.981823Z",
     "iopub.status.busy": "2025-08-29T08:38:13.981047Z",
     "iopub.status.idle": "2025-08-29T08:38:13.986078Z",
     "shell.execute_reply": "2025-08-29T08:38:13.985370Z",
     "shell.execute_reply.started": "2025-08-29T08:38:13.981798Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Model Training</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T06:16:00.490795Z",
     "iopub.status.busy": "2025-08-25T06:16:00.490221Z",
     "iopub.status.idle": "2025-08-25T06:19:10.637902Z",
     "shell.execute_reply": "2025-08-25T06:19:10.637180Z",
     "shell.execute_reply.started": "2025-08-25T06:16:00.490763Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Early Stopping + Checkpoint\n",
    "# =========================\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "patience = 10\n",
    "counter = 0\n",
    "checkpoint_path = \"best_swin_model.pth\"\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = 100. * correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss = running_loss / len(val_loader.dataset)\n",
    "    val_acc = 100. * correct / total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% \"\n",
    "          f\"| Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # ---- Checkpoint + Early Stopping ----\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"‚úÖ Best model updated & saved at epoch {epoch+1}\")\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"‚ö†Ô∏è No improvement for {counter} epochs\")\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(\"‚èπÔ∏è Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "print(\"‚úÖ Loaded best model from training\")\n",
    "\n",
    "# =========================\n",
    "# Save Training History\n",
    "# =========================\n",
    "history = {\n",
    "    'loss': train_losses,\n",
    "    'val_loss': val_losses,\n",
    "    'accuracy': train_accuracies,\n",
    "    'val_accuracy': val_accuracies\n",
    "}\n",
    "df_history = pd.DataFrame(history)\n",
    "df_history.to_csv(\"swin_training_history.csv\", index=False)\n",
    "print(\"üìÅ Training history saved to swin_training_history.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Results</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T06:19:17.867429Z",
     "iopub.status.busy": "2025-08-25T06:19:17.867091Z",
     "iopub.status.idle": "2025-08-25T06:19:21.920269Z",
     "shell.execute_reply": "2025-08-25T06:19:21.919429Z",
     "shell.execute_reply.started": "2025-08-25T06:19:17.867399Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Plot Training Curves\n",
    "# =========================\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 6))\n",
    "num_epochs_done = len(train_losses)\n",
    "xticks = range(0, num_epochs_done + 1, 2)\n",
    "\n",
    "# Accuracy\n",
    "axs[0].plot(train_accuracies, label='Train Accuracy')\n",
    "axs[0].plot(val_accuracies, label='Validation Accuracy')\n",
    "axs[0].set_xlabel('Epoch', fontsize=20)\n",
    "axs[0].set_ylabel('Accuracy (%)', fontsize=20)\n",
    "axs[0].set_title('Training and Validation Accuracy - Swin', fontsize=22)\n",
    "axs[0].legend(fontsize=18)\n",
    "axs[0].set_xticks(xticks)\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axs[1].plot(train_losses, label='Train Loss')\n",
    "axs[1].plot(val_losses, label='Validation Loss')\n",
    "axs[1].set_xlabel('Epoch', fontsize=20)\n",
    "axs[1].set_ylabel('Loss', fontsize=20)\n",
    "axs[1].set_title('Training and Validation Loss - Swin', fontsize=22)\n",
    "axs[1].legend(fontsize=18)\n",
    "axs[1].set_xticks(xticks)\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('swin_training_curves.png', dpi=600)\n",
    "plt.savefig('swin_training_curves.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T06:19:24.820964Z",
     "iopub.status.busy": "2025-08-25T06:19:24.819939Z",
     "iopub.status.idle": "2025-08-25T06:20:04.750979Z",
     "shell.execute_reply": "2025-08-25T06:20:04.750101Z",
     "shell.execute_reply.started": "2025-08-25T06:19:24.820936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Test Evaluation\n",
    "# =========================\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Performance Metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "rec = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "print(\"\\n===== üìä Test Performance =====\")\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(f\"MCC:       {mcc:.4f}\")\n",
    "\n",
    "# Save Performance Metrics\n",
    "metrics_df = pd.DataFrame([{\n",
    "    \"Accuracy\": acc,\n",
    "    \"Precision\": prec,\n",
    "    \"Recall\": rec,\n",
    "    \"F1-Score\": f1,\n",
    "    \"MCC\": mcc\n",
    "}])\n",
    "metrics_df.to_csv(\"swin_test_metrics.csv\", index=False)\n",
    "print(\"üìÅ Test metrics saved to swin_test_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T06:20:09.475874Z",
     "iopub.status.busy": "2025-08-25T06:20:09.475203Z",
     "iopub.status.idle": "2025-08-25T06:20:42.353590Z",
     "shell.execute_reply": "2025-08-25T06:20:42.353040Z",
     "shell.execute_reply.started": "2025-08-25T06:20:09.475844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(30, 25))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig('swin_confusion_matrix.png', dpi=600)\n",
    "plt.savefig('swin_confusion_matrix.pdf')\n",
    "plt.show()\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "df_cm.to_csv(\"confusion_swin.csv\")\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Plotting Predictions</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T04:09:01.564865Z",
     "iopub.status.busy": "2025-08-31T04:09:01.564310Z",
     "iopub.status.idle": "2025-08-31T04:09:07.909460Z",
     "shell.execute_reply": "2025-08-31T04:09:07.908426Z",
     "shell.execute_reply.started": "2025-08-31T04:09:01.564842Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "\n",
    "# =========================\n",
    "# 1) Rebuild model + load checkpoint\n",
    "# =========================\n",
    "num_classes = len(class_names)\n",
    "model = models.swin_t(weights=\"IMAGENET1K_V1\")\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "checkpoint_path = \"/kaggle/input/swin_10epochs/pytorch/default/1/best_swin_model.pth\"   # or your .h file\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.eval()\n",
    "print(\"‚úÖ Model loaded from checkpoint\")\n",
    "\n",
    "# =========================\n",
    "# 2) Run inference on test set\n",
    "# =========================\n",
    "correct_samples = []\n",
    "wrong_samples = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        for img, true, pred in zip(images, labels, preds):\n",
    "            if true == pred and len(correct_samples) < 4:\n",
    "                correct_samples.append((img.cpu(), true.item(), pred.item()))\n",
    "            elif true != pred and len(wrong_samples) < 4:\n",
    "                wrong_samples.append((img.cpu(), true.item(), pred.item()))\n",
    "\n",
    "        # stop once we got enough\n",
    "        if len(correct_samples) >= 4 and len(wrong_samples) >= 4:\n",
    "            break\n",
    "\n",
    "# =========================\n",
    "# 3) Helper to unnormalize image\n",
    "# =========================\n",
    "def imshow(img_tensor, title):\n",
    "    img = img_tensor.numpy().transpose((1, 2, 0))\n",
    "    img = np.clip(img, 0, 1)   # assuming transforms.ToTensor() only\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "# =========================\n",
    "# 4) Plot results\n",
    "# =========================\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i, (img, true, pred) in enumerate(correct_samples):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    imshow(img, f\"‚úÖ True: {class_names[true]}\\nPred: {class_names[pred]}\")\n",
    "\n",
    "for i, (img, true, pred) in enumerate(wrong_samples):\n",
    "    plt.subplot(2, 4, i+5)\n",
    "    imshow(img, f\"‚ùå True: {class_names[true]}\\nPred: {class_names[pred]}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-31T04:09:20.594427Z",
     "iopub.status.busy": "2025-08-31T04:09:20.593804Z",
     "iopub.status.idle": "2025-08-31T04:09:58.644186Z",
     "shell.execute_reply": "2025-08-31T04:09:58.643433Z",
     "shell.execute_reply.started": "2025-08-31T04:09:20.594401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# Collect Predictions\n",
    "# =========================\n",
    "model.eval()\n",
    "all_images, all_labels, all_preds = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Collecting predictions\"):\n",
    "        outputs = model(images.to(device))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        all_images.extend(images.cpu())   # keep CPU tensors for plotting\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# =========================\n",
    "# Visualization Function\n",
    "# =========================\n",
    "def plot_class_predictions(class_idx, correct=True, num_samples=4, save_path=None):\n",
    "    \"\"\"Plot correct or incorrect predictions for a given class index\"\"\"\n",
    "    imgs, titles = [], []\n",
    "\n",
    "    for img, label, pred in zip(all_images, all_labels, all_preds):\n",
    "        if label == class_idx:\n",
    "            if correct and label == pred:   # correct prediction\n",
    "                imgs.append(img)\n",
    "                titles.append((label, pred))\n",
    "            elif not correct and label != pred:  # incorrect prediction\n",
    "                imgs.append(img)\n",
    "                titles.append((label, pred))\n",
    "        if len(imgs) >= num_samples:\n",
    "            break\n",
    "    \n",
    "    if len(imgs) == 0:\n",
    "        print(f\"No {'correct' if correct else 'incorrect'} predictions found for class {class_names[class_idx]}\")\n",
    "        return\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, len(imgs), figsize=(15, 4))\n",
    "    fig.suptitle(\n",
    "        f\"{'Correct' if correct else 'Wrong'} Predictions: {class_names[class_idx]}\",\n",
    "        fontsize=16, weight=\"bold\"\n",
    "    )\n",
    "\n",
    "    for ax, img, (label, pred) in zip(axes, imgs, titles):\n",
    "        img = img.permute(1, 2, 0).numpy()\n",
    "        img = (img - img.min()) / (img.max() - img.min())  # normalize for display\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Actual: {class_names[label]}\\nPred: {class_names[pred]}\",\n",
    "                     fontsize=10)\n",
    "\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=600, bbox_inches=\"tight\")  # save before plt.show()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Example Usage\n",
    "# =========================\n",
    "# Suppose you want to check class index 5\n",
    "selected_class = 5\n",
    "\n",
    "# Save correct predictions\n",
    "plot_class_predictions(selected_class, correct=True, save_path=\"swin_correct.pdf\")\n",
    "\n",
    "# Save wrong predictions\n",
    "plot_class_predictions(selected_class, correct=False, save_path=\"swin_wrong.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Grad Cam Analysis</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T16:28:58.588377Z",
     "iopub.status.busy": "2025-08-30T16:28:58.588068Z",
     "iopub.status.idle": "2025-08-30T16:30:28.921466Z",
     "shell.execute_reply": "2025-08-30T16:30:28.920453Z",
     "shell.execute_reply.started": "2025-08-30T16:28:58.588355Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install grad-cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T17:03:12.690672Z",
     "iopub.status.busy": "2025-08-30T17:03:12.690012Z",
     "iopub.status.idle": "2025-08-30T17:03:14.442383Z",
     "shell.execute_reply": "2025-08-30T17:03:14.441623Z",
     "shell.execute_reply.started": "2025-08-30T17:03:12.690646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def visualize_gradcam(model, img_path, transform, device, class_names, target_layer, save_path=None, format='png'):\n",
    "    # Load and preprocess image\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Initialize Grad-CAM\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Generate heatmap\n",
    "    heatmap, pred_class = gradcam.generate(img_tensor)\n",
    "    \n",
    "    # Convert tensor to image\n",
    "    img_resized = img.resize((224, 224))\n",
    "    img_array = np.array(img_resized) / 255.0\n",
    "    \n",
    "    # Apply heatmap to image\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    heatmap = heatmap[..., ::-1]  # Convert BGR to RGB\n",
    "    \n",
    "    # Superimpose heatmap on original image\n",
    "    cam_img = heatmap + np.float32(img_array)\n",
    "    cam_img = cam_img / np.max(cam_img)\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    axes[0].imshow(img_array)\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(heatmap)\n",
    "    axes[1].set_title('Heatmap')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(cam_img)\n",
    "    axes[2].set_title(f'Grad-CAM: {class_names[pred_class]}')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure if save_path is provided\n",
    "    if save_path:\n",
    "        if format.lower() == 'pdf':\n",
    "            plt.savefig(save_path, format='pdf', bbox_inches='tight', dpi=300)\n",
    "        else:\n",
    "            plt.savefig(save_path, format='png', bbox_inches='tight', dpi=300)\n",
    "        print(f\"Saved visualization to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close(fig)  # Close the figure to free memory\n",
    "    \n",
    "    return pred_class\n",
    "\n",
    "# Now you can call it with save_path\n",
    "test_image_path = \"/kaggle/input/50-skin-disease/Best_50_class/Measles/Measles MSLD V2/mpx_fold1_Test_MSL_16_01.jpg\"\n",
    "pred_class = visualize_gradcam(\n",
    "    model, \n",
    "    test_image_path, \n",
    "    transform, \n",
    "    device, \n",
    "    class_names, \n",
    "    target_layer,\n",
    "    save_path=\"/kaggle/working/gradcam_singleImagevisualization.png\",  # This should work now\n",
    "    format='png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T17:06:09.260592Z",
     "iopub.status.busy": "2025-08-30T17:06:09.259966Z",
     "iopub.status.idle": "2025-08-30T17:06:16.253832Z",
     "shell.execute_reply": "2025-08-30T17:06:16.252862Z",
     "shell.execute_reply.started": "2025-08-30T17:06:09.260569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def visualize_gradcam_grid(model, dataloader, device, class_names, target_layer, num_images=9, save_path=None, format='png'):\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of images\n",
    "    images, labels = next(iter(dataloader))\n",
    "    images = images[:num_images].to(device)\n",
    "    labels = labels[:num_images].cpu().numpy()\n",
    "    \n",
    "    # Initialize Grad-CAM\n",
    "    gradcam = GradCAM(model, target_layer)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Generate heatmap\n",
    "        heatmap, pred_class = gradcam.generate(images[i:i+1])\n",
    "        \n",
    "        # Convert tensor to image\n",
    "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
    "        img = (img - img.min()) / (img.max() - img.min())\n",
    "        \n",
    "        # Apply heatmap to image\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "        heatmap = np.float32(heatmap) / 255\n",
    "        heatmap = heatmap[..., ::-1]  # Convert BGR to RGB\n",
    "        \n",
    "        # Superimpose heatmap on original image\n",
    "        cam_img = heatmap + np.float32(img)\n",
    "        cam_img = cam_img / np.max(cam_img)\n",
    "        \n",
    "        # Plot\n",
    "        axes[i].imshow(cam_img)\n",
    "        axes[i].set_title(f'True: {class_names[labels[i]]}\\nPred: {class_names[pred_class]}', fontsize=10)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure if save_path is provided\n",
    "    if save_path:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        \n",
    "        if format.lower() == 'pdf':\n",
    "            plt.savefig(save_path, format='pdf', bbox_inches='tight', dpi=300)\n",
    "        else:\n",
    "            plt.savefig(save_path, format='png', bbox_inches='tight', dpi=300)\n",
    "        print(f\"‚úÖ Saved grid visualization to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close(fig)  # Close the figure to free memory\n",
    "\n",
    "# Usage example\n",
    "visualize_gradcam_grid(\n",
    "    model, \n",
    "    test_loader, \n",
    "    device, \n",
    "    class_names, \n",
    "    target_layer, \n",
    "    num_images=9,\n",
    "    save_path=\"/kaggle/working/gradcam_resultsGridImage.png\",\n",
    "    format='png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------\n",
    "# Load your model\n",
    "# --------------------\n",
    "num_classes = len(class_names)\n",
    "model = models.swin_t(weights=\"IMAGENET1K_V1\")\n",
    "model.head = nn.Linear(model.head.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "checkpoint_path = \"/kaggle/input/swin_10epochs/pytorch/default/1/best_swin_model.pth\"   # or your .h file\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.eval()\n",
    "print(\"‚úÖ Model loaded from checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T17:50:59.896921Z",
     "iopub.status.busy": "2025-08-30T17:50:59.896089Z",
     "iopub.status.idle": "2025-08-30T17:50:59.905852Z",
     "shell.execute_reply": "2025-08-30T17:50:59.904963Z",
     "shell.execute_reply.started": "2025-08-30T17:50:59.896893Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Print model structure to find the right layer\n",
    "print(model.features)\n",
    "\n",
    "# Try different layers - common options for Swin Transformer\n",
    "# Option 1: Last layer of the feature extractor\n",
    "target_layer = model.features[-1][-1]\n",
    "\n",
    "# Option 2: Last normalization layer\n",
    "for name, module in model.named_modules():\n",
    "    if 'norm' in name and 'head' not in name:\n",
    "        print(f\"Found norm layer: {name}\")\n",
    "        target_layer = module\n",
    "\n",
    "# Option 3: Last attention block\n",
    "for name, module in model.named_modules():\n",
    "    if 'attention' in name:\n",
    "        print(f\"Found attention layer: {name}\")\n",
    "        target_layer = module\n",
    "\n",
    "# Register hooks\n",
    "target_layer.register_forward_hook(forward_hook)\n",
    "target_layer.register_backward_hook(backward_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-30T17:52:15.092663Z",
     "iopub.status.busy": "2025-08-30T17:52:15.091875Z",
     "iopub.status.idle": "2025-08-30T17:52:16.125740Z",
     "shell.execute_reply": "2025-08-30T17:52:16.124912Z",
     "shell.execute_reply.started": "2025-08-30T17:52:15.092638Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Define preprocessing\n",
    "# --------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --------------------\n",
    "# Grad-CAM hook setup - CORRECTED\n",
    "# --------------------\n",
    "activations = {}\n",
    "gradients = {}\n",
    "\n",
    "def forward_hook(module, input, output):\n",
    "    activations['value'] = output\n",
    "\n",
    "def backward_hook(module, grad_in, grad_out):\n",
    "    gradients['value'] = grad_out[0]\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Grad-CAM function\n",
    "# --------------------\n",
    "def compute_gradcam(img_tensor, class_idx=None):\n",
    "    img_tensor = img_tensor.to(device)\n",
    "\n",
    "    # Forward\n",
    "    output = model(img_tensor.unsqueeze(0))  # Add batch dimension\n",
    "    if class_idx is None:\n",
    "        class_idx = output.argmax(dim=1).item()\n",
    "\n",
    "    # Backward\n",
    "    model.zero_grad()\n",
    "    output[0, class_idx].backward()\n",
    "\n",
    "    # Get activations & gradients\n",
    "    act = activations['value'].detach().cpu()    # [B, C, H, W]\n",
    "    grad = gradients['value'].detach().cpu()     # [B, C, H, W]\n",
    "    \n",
    "    # Global average pooling of gradients\n",
    "    weights = grad.mean(dim=(2, 3), keepdim=True)  # [B, C, 1, 1]\n",
    "    \n",
    "    # Weight the activation maps\n",
    "    cam = (weights * act).sum(dim=1)  # [B, H, W]\n",
    "    cam = cam[0].numpy()  # Get the first image in batch\n",
    "    \n",
    "    # Normalize CAM\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / (cam.max() + 1e-8)  # Avoid division by zero\n",
    "    \n",
    "    # Resize to input size\n",
    "    cam = cv2.resize(cam, (224, 224))\n",
    "    \n",
    "    return cam, class_idx\n",
    "\n",
    "def overlay_gradcam(cam, img_tensor):\n",
    "    img = img_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    heatmap = heatmap[..., ::-1]  # Convert BGR to RGB\n",
    "    \n",
    "    # Superimpose heatmap on original image\n",
    "    overlay = 0.5 * heatmap + 0.5 * img\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    return overlay\n",
    "\n",
    "# --------------------\n",
    "# Example usage\n",
    "# --------------------\n",
    "img_path = \"/kaggle/input/50-skin-disease/Best_50_class/Measles/Measles MSLD V2/mpx_fold1_Test_MSL_16_01.jpg\"\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img_tensor = transform(img)\n",
    "\n",
    "cam, pred = compute_gradcam(img_tensor)\n",
    "overlay = overlay_gradcam(cam, img_tensor)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cam, cmap='jet')\n",
    "plt.title(\"Grad-CAM Heatmap\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(overlay)\n",
    "plt.title(f\"Overlay (Predicted: {class_names[pred]})\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"gradcam_result.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Vit-Pytorch</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Importing Libraries</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T06:21:07.316384Z",
     "iopub.status.busy": "2025-08-25T06:21:07.316049Z",
     "iopub.status.idle": "2025-08-25T06:21:07.320147Z",
     "shell.execute_reply": "2025-08-25T06:21:07.319438Z",
     "shell.execute_reply.started": "2025-08-25T06:21:07.316358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Model Compiling</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T06:21:09.065890Z",
     "iopub.status.busy": "2025-08-25T06:21:09.065493Z",
     "iopub.status.idle": "2025-08-25T06:21:12.387842Z",
     "shell.execute_reply": "2025-08-25T06:21:12.387285Z",
     "shell.execute_reply.started": "2025-08-25T06:21:09.065861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Define ViT Model\n",
    "# =========================\n",
    "model = models.vit_b_16(weights=\"IMAGENET1K_V1\")  # Pretrained ViT-Base, patch 16\n",
    "model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)  # Replace classification head\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T06:21:12.843991Z",
     "iopub.status.busy": "2025-08-25T06:21:12.843449Z",
     "iopub.status.idle": "2025-08-25T06:21:12.850582Z",
     "shell.execute_reply": "2025-08-25T06:21:12.849829Z",
     "shell.execute_reply.started": "2025-08-25T06:21:12.843968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Model Training</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T06:21:14.687820Z",
     "iopub.status.busy": "2025-08-25T06:21:14.687509Z",
     "iopub.status.idle": "2025-08-25T06:28:52.544088Z",
     "shell.execute_reply": "2025-08-25T06:28:52.543218Z",
     "shell.execute_reply.started": "2025-08-25T06:21:14.687796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Early Stopping + Checkpoint (same as before)\n",
    "# =========================\n",
    "best_val_loss = float(\"inf\")\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "patience = 5\n",
    "counter = 0\n",
    "checkpoint_path = \"best_vit_model.pth\"\n",
    "\n",
    "num_epochs = 50\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = 100. * correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss = running_loss / len(val_loader.dataset)\n",
    "    val_acc = 100. * correct / total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% \"\n",
    "          f\"| Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # ---- Checkpoint + Early Stopping ----\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"‚úÖ Best model updated & saved at epoch {epoch+1}\")\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"‚ö†Ô∏è No improvement for {counter} epochs\")\n",
    "        if counter >= patience:\n",
    "            print(\"‚èπÔ∏è Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# Load best model weights\n",
    "model.load_state_dict(best_model_wts)\n",
    "print(\"‚úÖ Loaded best ViT model from training\")\n",
    "# =========================\n",
    "# Save Training History\n",
    "# =========================\n",
    "history = {\n",
    "    'loss': train_losses,\n",
    "    'val_loss': val_losses,\n",
    "    'accuracy': train_accuracies,\n",
    "    'val_accuracy': val_accuracies\n",
    "}\n",
    "df_history = pd.DataFrame(history)\n",
    "df_history.to_csv(\"vit_training_history.csv\", index=False)\n",
    "print(\"üìÅ Training history saved to vit_training_history.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Results</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T06:29:00.680012Z",
     "iopub.status.busy": "2025-08-25T06:29:00.679687Z",
     "iopub.status.idle": "2025-08-25T06:29:03.540248Z",
     "shell.execute_reply": "2025-08-25T06:29:03.539535Z",
     "shell.execute_reply.started": "2025-08-25T06:29:00.679983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Plot Training Curves\n",
    "# =========================\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 6))\n",
    "num_epochs_done = len(train_losses)\n",
    "xticks = range(0, num_epochs_done + 1, 2)\n",
    "\n",
    "# Accuracy\n",
    "axs[0].plot(train_accuracies, label='Train Accuracy')\n",
    "axs[0].plot(val_accuracies, label='Validation Accuracy')\n",
    "axs[0].set_xlabel('Epoch', fontsize=20)\n",
    "axs[0].set_ylabel('Accuracy (%)', fontsize=20)\n",
    "axs[0].set_title('Training and Validation Accuracy - Vision Transformer', fontsize=22)\n",
    "axs[0].legend(fontsize=18)\n",
    "axs[0].set_xticks(xticks)\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axs[1].plot(train_losses, label='Train Loss')\n",
    "axs[1].plot(val_losses, label='Validation Loss')\n",
    "axs[1].set_xlabel('Epoch', fontsize=20)\n",
    "axs[1].set_ylabel('Loss', fontsize=20)\n",
    "axs[1].set_title('Training and Validation Loss - Vision Transformer', fontsize=22)\n",
    "axs[1].legend(fontsize=18)\n",
    "axs[1].set_xticks(xticks)\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('vitTorch_training_curves.png', dpi=600)\n",
    "plt.savefig('vitTorch_training_curves.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T06:29:04.355765Z",
     "iopub.status.busy": "2025-08-25T06:29:04.355044Z",
     "iopub.status.idle": "2025-08-25T06:29:35.864969Z",
     "shell.execute_reply": "2025-08-25T06:29:35.864212Z",
     "shell.execute_reply.started": "2025-08-25T06:29:04.355741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Test Evaluation\n",
    "# =========================\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Performance Metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "rec = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "print(\"\\n===== üìä Test Performance =====\")\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(f\"MCC:       {mcc:.4f}\")\n",
    "\n",
    "# Save Performance Metrics\n",
    "metrics_df = pd.DataFrame([{\n",
    "    \"Accuracy\": acc,\n",
    "    \"Precision\": prec,\n",
    "    \"Recall\": rec,\n",
    "    \"F1-Score\": f1,\n",
    "    \"MCC\": mcc\n",
    "}])\n",
    "metrics_df.to_csv(\"vitTorch_test_metrics.csv\", index=False)\n",
    "print(\"üìÅ Test metrics saved to vitTorch_test_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-25T06:29:35.866661Z",
     "iopub.status.busy": "2025-08-25T06:29:35.866438Z",
     "iopub.status.idle": "2025-08-25T06:30:31.819045Z",
     "shell.execute_reply": "2025-08-25T06:30:31.818397Z",
     "shell.execute_reply.started": "2025-08-25T06:29:35.866638Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(30, 25))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig('vitTorch_confusion_matrix.png', dpi=600)\n",
    "plt.savefig('vitTorch_confusion_matrix.pdf')\n",
    "plt.show()\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "df_cm.to_csv(\"confusion_vitTorch.csv\")\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>Zipping all files</b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r /kaggle/working/output_files_swin_vit.zip /kaggle/working/*"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8131776,
     "sourceId": 12856417,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 439066,
     "modelInstanceId": 421451,
     "sourceId": 552545,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
